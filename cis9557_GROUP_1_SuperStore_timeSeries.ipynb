{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTNFs+c+u4++bXdQOW20A1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CIS9557_bctw/blob/main/cis9557_GROUP_1_SuperStore_timeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CIS9557 SuperStore Time Series  \n",
        "<b>Scaffolding and Data Wrangling for Group 1</b>  \n",
        "<b>Using `Ship Date`, and a.) `Category`, or b.) `Region` Features</b>  \n",
        "  \n",
        "*example of data wrangling for time-series, forecasting*  \n",
        "\n"
      ],
      "metadata": {
        "id": "_GDBPCDkk_TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0. Housekeeping  "
      ],
      "metadata": {
        "id": "dpyPdaVgnen6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the usual suspects"
      ],
      "metadata": {
        "id": "IUF1wtcQng7f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u-4bYmOskszB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "RozwIo7_nlgy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "UR5oyJWjuBhs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "qvO654munoZW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load (US) sample Superstore.XLS Orders spreadsheet from CSV file on Professor Patrick's Github  \n"
      ],
      "metadata": {
        "id": "YmExrkdrqECB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl 'https://raw.githubusercontent.com/ProfessorPatrickSlatraigh/data/main/USsampleSuperstoreOrders_May-2024.csv' -o usOrders.csv"
      ],
      "metadata": {
        "id": "7865Sk52qRXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22610625-ed23-44df-b189-f12c0c50d873"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2234k  100 2234k    0     0  4332k      0 --:--:-- --:--:-- --:--:-- 4339k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tmKoDXCRqw0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Create a Date Range Scaffold  \n",
        "  \n",
        "Spin up a series of `timedate` values to use as scaffolding for time series analysis  \n"
      ],
      "metadata": {
        "id": "wX2_dASMqxn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def timescaffold(startIncludes='01-Jan-2019', endIncludes='31-Mar-2019', periodFrequency='daily'):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "      startIncludes:   the scaffold series starting date in %d-%b-$Y (dd-Mmm-YYYY) format\n",
        "      endIncludes:     the scaffold series ending date in %d-%b-$Y (dd-Mmm-YYYY) format\n",
        "      periodFrequency: the frequency of the scaffold series periods (daily, weekdays, businessdays, weekly, monthly, annually)\n",
        "    \"\"\"\n",
        "\n",
        "    print(f'From {startIncludes} to {endIncludes} at a {periodFrequency} rate.\\n')\n",
        "\n",
        "    # set a pattern for the expected date format in strings (user entry)\n",
        "    date_format = '%d-%b-%Y'  # %d for day, %b for abbreviated month, %Y for four-digit year\n",
        "    # test for a missing start date parameter and prompt for a valid value\n",
        "    # if startIncludes == '01-Jan-2019':\n",
        "    goodStart = input(f'\\nThe time series will start with {startIncludes}. Is that correct? ')\n",
        "    while goodStart.lower()[0] != 'y':\n",
        "        startIncludes = input('What value would you like to start with? ')\n",
        "        try:\n",
        "            startScaffold = datetime.strptime(startIncludes, date_format)\n",
        "            goodStart='yes'\n",
        "        except:\n",
        "            print('Your input was not a valid date in the format \"dd-Mmm-YYYY\". Enter a new value.')\n",
        "    startScaffold = datetime.strptime(startIncludes, date_format)\n",
        "    print(f'Thank you.  The scaffold time series will begin with {startScaffold}.')\n",
        "    # test for a missing end date parameter and prompt for a valid value\n",
        "    # if endIncludes == '31-Mar-2019':\n",
        "    goodEnd = input(f'\\nThe time series will end with {endIncludes}. Is that correct? ')\n",
        "    while goodEnd.lower()[0] != 'y':\n",
        "        endIncludes = input('What value would you like to start with?')\n",
        "        try:\n",
        "            endScaffold = datetime.strptime(endIncludes, date_format)\n",
        "            goodEnd = 'yes'\n",
        "        except:\n",
        "            print('Your input was not a valid date in the format \"dd-Mmm-YYYY\". Enter a new value. ')\n",
        "    endScaffold = datetime.strptime(endIncludes, date_format)\n",
        "    print(f'Thank you.  The scaffold time series will end with {datetime.strptime(endIncludes, date_format)}.')\n",
        "\n",
        "    return(pd.date_range(start=startScaffold, end=endScaffold, freq='D'))\n",
        "\n"
      ],
      "metadata": {
        "id": "hICPNjc4rJ0b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaffold = timescaffold()"
      ],
      "metadata": {
        "id": "seNIyjIuy5oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10e4c37-c99f-4947-d30a-796d95febbee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From 01-Jan-2019 to 31-Mar-2019 at a daily rate.\n",
            "\n",
            "\n",
            "The time series will start with 01-Jan-2019. Is that correct? y\n",
            "Thank you.  The scaffold time series will begin with 2019-01-01 00:00:00.\n",
            "\n",
            "The time series will end with 31-Mar-2019. Is that correct? y\n",
            "Thank you.  The scaffold time series will end with 2019-03-31 00:00:00.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(scaffold))"
      ],
      "metadata": {
        "id": "b6rnlB-o63Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scaffold)"
      ],
      "metadata": {
        "id": "UZgv5_1h6j_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4NCwUXO-gkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load Profit Dataframe  \n",
        "  \n",
        "Create a `Dataframe` of the `ShippedDate` and `Profit` variables (columns) from the `Orders` dataset (sheet) retaining the following features:  \n",
        "- `Row ID`  \n",
        "- `Ship Date`  \n",
        "- `Segment`  \n",
        "- `State`  \n",
        "- `Region`  \n",
        "- `Product ID`  \n",
        "- `Category`  \n",
        "- `Profit` (target)  \n",
        "  "
      ],
      "metadata": {
        "id": "A7w9SbP_7BsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a list of the columns to keep and the names to use for them"
      ],
      "metadata": {
        "id": "aaarhp3W8MFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keep_columns_ls = ['Row ID', 'Ship Date', 'Category', 'Segment', 'State', 'Region', 'Product ID', 'Category', 'Profit']"
      ],
      "metadata": {
        "id": "JOlhF7378iQg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = pd.read_csv('usOrders.csv', usecols=keep_columns_ls)"
      ],
      "metadata": {
        "id": "xN2nfIHS7cn-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.columns"
      ],
      "metadata": {
        "id": "ASrUkqft7xPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.describe()"
      ],
      "metadata": {
        "id": "b6kLXCOa7qk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `datetime` value on each row from its  'Ship Date'"
      ],
      "metadata": {
        "id": "ed08PsdT9tTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df['DateShipped'] = pd.to_datetime(orders_df['Ship Date'])"
      ],
      "metadata": {
        "id": "rVgxl-T092Pu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.head(15)"
      ],
      "metadata": {
        "id": "uLTSth6X-QXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Merge Timedate Scaffold and Superstore Dataframe  \n",
        "\n",
        "Merge the time series scaffold with the dataset in the `dataframe`  "
      ],
      "metadata": {
        "id": "H3fJf5nC-7TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaffold_df = pd.DataFrame(scaffold, columns=['Date'])"
      ],
      "metadata": {
        "id": "d29f5r9h_EfM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaffold_df.head(15)"
      ],
      "metadata": {
        "id": "nfgRDSIQ_Jpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(scaffold_df)"
      ],
      "metadata": {
        "id": "WNrHWK1bCt7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.a) Enrich Scaffolding for `Ship Date` and `Region`  \n",
        "  \n",
        "<font color=blue><i>Note: the steps in the section are to scaffold the Superstore dataset for `Ship Date` and `Region` and produce a dataset that has at least one row (record) for every combination of `Ship Date` and `Region`.  If the original dataset has more than one row for any combination of `Ship Date` and `Region` those will be aggregated so that we have only one row for every combination of desired features -- that one row will have the aggregate profit for that combination of features.</i></font>  \n",
        "  \n",
        "<font color=red><i>If you intend to analyze the dataset by `Ship Date` and `Category`, then use <b>Step 3.b)</b> instead of this step.</i></font>\n",
        "\n"
      ],
      "metadata": {
        "id": "m4qU98oVSHXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `regions_df` `dataframe` with a value for every `Region`"
      ],
      "metadata": {
        "id": "s42uVpctF7ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame with the 'Region' values\n",
        "regions = ['West', 'East', 'Central', 'South']\n",
        "regions_df = pd.DataFrame({'Region': regions})"
      ],
      "metadata": {
        "id": "ejtyxxO3DML4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing a cross join to combine every 'Date' with every 'Region'\n",
        "expandedScaffold_df = scaffold_df.assign(key=1).merge(regions_df.assign(key=1), on='key').drop('key', axis=1)"
      ],
      "metadata": {
        "id": "8dwCkPfZGQ39"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expandedScaffold_df.head(10)"
      ],
      "metadata": {
        "id": "zOgpR4ykIW-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(expandedScaffold_df)"
      ],
      "metadata": {
        "id": "2al-QoaUCxFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = pd.merge(expandedScaffold_df, orders_df, left_on=['Date', 'Region'], right_on=['DateShipped', 'Region'], how='outer').drop('DateShipped', axis=1)"
      ],
      "metadata": {
        "id": "xLbU6E4iCRY9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have an enriched dataset with all the `Date Shipped` and `Region` combinations included at least once, but some of these record may have missing values (NaN) for `Profit`.  "
      ],
      "metadata": {
        "id": "TcG3FdpSXwwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.head(20)"
      ],
      "metadata": {
        "id": "CQ1M6qlh_-dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_df)"
      ],
      "metadata": {
        "id": "jfQ_zQ9xATFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-total the `Profit` data in the `training_df` dataset by `Region` and `Date`"
      ],
      "metadata": {
        "id": "EC4m2fwwA8zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a specific column for more meaningful count, here using 'Value' column\n",
        "summaryProfit_df = training_df.groupby(['Region', 'Date'])['Profit'].count().reset_index(name='Count')"
      ],
      "metadata": {
        "id": "NU_fHt4jA0Sw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(summaryProfit_df)"
      ],
      "metadata": {
        "id": "xaVhQi-fB-4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "print(summaryProfit_df)"
      ],
      "metadata": {
        "id": "O9YMuMxKA7-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-total the `Date` instances in the `training_df` dataset by `Region` and `Date`"
      ],
      "metadata": {
        "id": "iSrDx5snBg4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a specific column for more meaningful count, here using 'Value' column\n",
        "summaryDate_df = training_df.groupby(['Region', 'Date'])['Date'].count().reset_index(name='Count')"
      ],
      "metadata": {
        "id": "V_f9mdi5BgfT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(summaryDate_df)"
      ],
      "metadata": {
        "id": "-VU1tYkJCBiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "print(summaryDate_df)"
      ],
      "metadata": {
        "id": "IMhUVibIB8h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><b><font color=blue>NOW PROCEED TO STEP #4</b></font></center>  \n"
      ],
      "metadata": {
        "id": "Fqu1BN1YedC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Vw8_sj9fecP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.b) Enrich Scaffolding for `Ship Date` and `Category`  \n",
        "  \n",
        "<font color=blue><i>Note: the steps in the section are to scaffold the Superstore dataset for `Ship Date` and `Category` and produce a dataset that has at least one row (record) for every combination of `Ship Date` and `Category`.  If the original dataset has more than one row for any combination of `Ship Date` and `Category` those will be aggregated so that we have only one row for every combination of desired features -- that one row will have the aggregate profit for that combination of features.</i></font>  \n",
        "  \n",
        "<font color=red><i>If you intend to analyze the dataset by `Ship Date` and `Region`, then use <b>Step 3.a)</b> instead of this step.</i></font>\n",
        "\n"
      ],
      "metadata": {
        "id": "nX585xSZV9dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a `categories_df` `dataframe` with a value for every `Category`"
      ],
      "metadata": {
        "id": "lVzoBt_CV9dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame with the 'Category' values\n",
        "categories = ['Furniture', 'Office Supplies', 'Technology']\n",
        "categories_df = pd.DataFrame({'Category': categories})"
      ],
      "metadata": {
        "id": "yI29ylhlV9dU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing a cross join to combine every 'Date' with every 'Catgeory'\n",
        "expandedScaffold_df = scaffold_df.assign(key=1).merge(categories_df.assign(key=1), on='key').drop('key', axis=1)"
      ],
      "metadata": {
        "id": "yQhiylfzV9dU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expandedScaffold_df.head(10)"
      ],
      "metadata": {
        "id": "_9xDvvAbV9dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(expandedScaffold_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPtBwbSIV9dU",
        "outputId": "cb222444-50b5-41e7-e955-11853ef60141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = pd.merge(expandedScaffold_df, orders_df, left_on=['Date', 'Category'], right_on=['DateShipped', 'Category'], how='outer').drop('DateShipped', axis=1)"
      ],
      "metadata": {
        "id": "tIYZCIz6V9dU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have an enriched dataset with all the `Date Shipped` and `Category` combinations included at least once, but some of these record may have missing values (NaN) for `Profit`.  "
      ],
      "metadata": {
        "id": "b0HxncqJXeKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.head(20)"
      ],
      "metadata": {
        "id": "oOxXFjjkV9dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_df)"
      ],
      "metadata": {
        "id": "cXR_emLbV9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-total the `Profit` data in the `training_df` dataset by `Category` and `Date`"
      ],
      "metadata": {
        "id": "E9gVaxr0V9dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a specific column for more meaningful count, here using 'Value' column\n",
        "summaryProfit_df = training_df.groupby(['Category', 'Date'])['Profit'].count().reset_index(name='Count')"
      ],
      "metadata": {
        "id": "tK9mpKMTV9dV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(summaryProfit_df)"
      ],
      "metadata": {
        "id": "Z6hjdVOhV9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "print(summaryProfit_df)"
      ],
      "metadata": {
        "id": "05kKuH16V9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-total the `Date` instances in the `training_df` dataset by `Category` and `Date`"
      ],
      "metadata": {
        "id": "tnDhF5bKV9dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a specific column for more meaningful count, here using 'Value' column\n",
        "summaryDate_df = training_df.groupby(['Category', 'Date'])['Date'].count().reset_index(name='Count')"
      ],
      "metadata": {
        "id": "tQAwfMapV9dV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(summaryDate_df)"
      ],
      "metadata": {
        "id": "lHLt19AYV9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result\n",
        "print(summaryDate_df)"
      ],
      "metadata": {
        "id": "QYjktx-kV9dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5HUF6EUKY59S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><b><font color=blue>Regardless of whether sub-step 3.a or 3.b above was used, all of the following steps apply.</b></font></center>"
      ],
      "metadata": {
        "id": "iM0dgNfKY7O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3f8XL5W5JLat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Fill Missing Values"
      ],
      "metadata": {
        "id": "U2XpggjRKEmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can replace missing (NaN) values for `Profit` with zeroes.  Those will typically occur on the new, scaffolder rows we created for feature combinations with `Ship Date` that did not occur in the original dataset."
      ],
      "metadata": {
        "id": "reD6WX5KZ9nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.columns"
      ],
      "metadata": {
        "id": "pgEvFH82KM1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.describe()"
      ],
      "metadata": {
        "id": "XopSR1L3KTY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df['Profit'].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "fJzvni7qKHOn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KxiCb43kZ8Fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Write CSV File"
      ],
      "metadata": {
        "id": "l5Ham9-lJFBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We write the resulting file as `training.csv` to the current working directory.  "
      ],
      "metadata": {
        "id": "HVl3TQoQa45g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.to_csv('training.csv', sep=',', header=True, mode='w', encoding='utf-8')"
      ],
      "metadata": {
        "id": "WkxwDPgrJKfn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XL8beQLL-eYT"
      }
    }
  ]
}