{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOTvynHd6tjkY3cGZMMOYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CIS9557_bctw/blob/main/cis9557_IBM_HR_attrition_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IBM HR Attrition dataset  \n",
        "\n",
        "<b>by Professor Patrick, June 2024</b>  \n",
        "  \n",
        "The IBM HR Analytics Employee Attrition & Performance dataset is commonly used to demonstrate various business analytics techniques.\n",
        "  \n",
        "This dataset includes features like employee age, attrition, department, distance from home, education, job involvement, job level, job satisfaction, and other relevant variables that can be analyzed to understand factors influencing employee retention and performance.  "
      ],
      "metadata": {
        "id": "eFlItC9zF5aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IBM HR Analytics Employee Attrition & Performance dataset is an excellent resource for developing a predictive analytics example. Here are several reasons why this dataset is particularly useful:  \n",
        "\n",
        "- <b>Real-World Relevance</b>: The dataset contains realistic and comprehensive employee data that mirrors what students might encounter in a real-world business environment.  \n",
        "  \n",
        "- <b>Diverse Features</b>: It includes various features (age, job role, department, salary, work-life balance, job satisfaction, etc.) that can be used to build complex models and understand different factors affecting employee attrition.  \n",
        "  \n",
        "- <b>Predictive Modelling</b>: Students can apply various predictive analytics techniques such as logistic regression, decision trees, random forests, and support vector machines to predict employee attrition.  \n",
        "  \n",
        "- <b>Data Preprocessing</b>: The dataset offers opportunities to teach students about data cleaning, feature selection, and transformationâ€”key steps in any predictive analytics project.  \n",
        "  \n",
        "- <b>Interpretability</b>: The results from the predictive models can be interpreted and communicated effectively, which is essential for making data-driven business decisions.  \n"
      ],
      "metadata": {
        "id": "33A7OIU4Gylr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before building a predictive analytics model, we can use the IBM HR Attrition dataset to conduct some descriptive analytics."
      ],
      "metadata": {
        "id": "rrmbHRSV3jiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gKOnFo0KZPpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Housekeeping  "
      ],
      "metadata": {
        "id": "zSOtCOhkGZF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare for the steps that we will execute, we first import some open-source modules of Python code.  That is, program code developed and maintained by others which we are able to use freely.  The modules we will incorporate in our steps include:\n",
        "\n",
        "- <b>Pandas</b> - for organizing data into two-dimensional DataFrames  \n",
        "- <b>SciKit-Learn</b> (sklearn) - for predictive modeling routines  \n",
        "- <b>Matplotlib</b> - to draw plots (charts)  \n",
        "- <b>Numpy</b> - for arithmetic formulae  \n",
        "\n"
      ],
      "metadata": {
        "id": "oZL9t4f-hUwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas for dataframes\n",
        "import pandas as pd\n",
        "\n",
        "# SciKit Learn for modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# pylot for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# numpy for numerical operations\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s43QrxuzLGJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AUWf-_BBZNqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sourcing Data"
      ],
      "metadata": {
        "id": "91XRS5_gLKbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source the dataset\n",
        "!curl https://raw.githubusercontent.com/ProfessorPatrickSlatraigh/data/main/IBM_Fn-UseC_-HR-Employee-Attrition.csv -o \"IBM_HR_Employee_Attrition.csv\"\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('IBM_HR_Employee_Attrition.csv')"
      ],
      "metadata": {
        "id": "yzwR-JNKFdeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "27YSttiTZMZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive Analytics  \n",
        "  \n",
        "Here's some code to perform a simple descriptive analysis of the `IBM HR Analytics Employee Attrition & Performance dataset`. This analysis includes loading the dataset, displaying basic information, checking for missing values, and summarizing key statistics for numerical and categorical features.  \n",
        "\n",
        "\n",
        "1. **Displaying the First Few Rows**:\n",
        "   - `print(data.head())`: This prints the first few rows of the dataset to give an initial view of the data.\n",
        "\n",
        "2. **Basic Information**:\n",
        "   - `print(data.info())`: This provides a summary of the dataset, including the number of non-null entries and data types of each column.\n",
        "\n",
        "3. **Checking for Missing Values**:\n",
        "   - `print(data.isnull().sum())`: This checks and displays the number of missing values in each column.\n",
        "\n",
        "4. **Summary Statistics for Numerical Features**:\n",
        "   - `print(data.describe())`: This provides summary statistics (count, mean, std, min, 25%, 50%, 75%, max) for the numerical features.\n",
        "\n",
        "5. **Summary Statistics for Categorical Features**:\n",
        "   - `print(data.describe(include=['object']))`: This provides summary statistics for categorical features, including the count, unique values, top value, and frequency.\n",
        "\n",
        "6. **Distribution of the Target Variable (Attrition)**:\n",
        "   - `print(data['Attrition'].value_counts())`: This displays the distribution of the target variable, showing the counts of employees who stayed and left.\n",
        "\n",
        "7. **Distribution of Categorical Features**:\n",
        "   - A loop iterates over each categorical feature to display its distribution using `value_counts()`.\n"
      ],
      "metadata": {
        "id": "BRVcnQ87RkET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Exploration  \n"
      ],
      "metadata": {
        "id": "k_ZOfUvuSNv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nBasic information about the dataset:\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "id": "oo9Y9KW_R6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descriptive Analysis - Counts   \n",
        "  "
      ],
      "metadata": {
        "id": "Xq4gOPRxSWIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values in the dataset:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Display summary statistics for numerical features\n",
        "print(\"\\nSummary statistics for numerical features:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Display summary statistics for categorical features\n",
        "print(\"\\nSummary statistics for categorical features:\")\n",
        "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
        "for feature in categorical_features:\n",
        "    print(f\"\\n{feature} distribution:\")\n",
        "    print(data[feature].value_counts())\n",
        "if categorical_features.empty:\n",
        "    print('No categorical features found.')\n"
      ],
      "metadata": {
        "id": "26sPBC49SalS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Target Variable"
      ],
      "metadata": {
        "id": "H2xKUE8nSfo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the distribution of the target variable (Attrition)\n",
        "print(\"\\nDistribution of the target variable (Attrition):\")\n",
        "print(data['Attrition'].value_counts())"
      ],
      "metadata": {
        "id": "ZNyednUMStR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exploring Categorical Variables  \n",
        "  "
      ],
      "metadata": {
        "id": "8fgPU1DnSxQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the distribution of categorical features\n",
        "print(\"\\nDistribution of categorical features:\")\n",
        "categorical_features = data.select_dtypes(include=['object']).columns\n",
        "for feature in categorical_features:\n",
        "    print(f\"\\n{feature} distribution:\")\n",
        "    print(data[feature].value_counts())"
      ],
      "metadata": {
        "id": "OTVbB02LS0ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eoTB3Dr3ZIDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations"
      ],
      "metadata": {
        "id": "ybwH2m38GjoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can separate our feautures from the target variable (attrition) and convert our target variable to a binary (1=True, 0=False).   "
      ],
      "metadata": {
        "id": "vr50n5G5huAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable\n",
        "X = data.drop('Attrition', axis=1)  # Assuming 'Attrition' is the target variable\n",
        "y = data['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)  # Convert to binary"
      ],
      "metadata": {
        "id": "b9JlgYNJhy9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform the following transformations next:  \n",
        "\n",
        "- identify the numerical and categorical values  \n",
        "- create and apply a columnart transformar  \n",
        "- create a pipeline of preprocesser and Logistic Regression  \n",
        "  "
      ],
      "metadata": {
        "id": "Z65llxLUiYCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()"
      ],
      "metadata": {
        "id": "8tgv1RZyidAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column transformer with appropriate transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a pipeline with the preprocessor and the logistic regression model\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', LogisticRegression())\n",
        "])"
      ],
      "metadata": {
        "id": "2uxRIJ2djBwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reserve 30% of the data to be used as the test set, while the remaining 70% will be used as the training set.  In other words, 70% of the data will be used to develop the model and the remaining 30% of the data can be used to test the model."
      ],
      "metadata": {
        "id": "bkepvDGkIuJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "2UJwofh5FiJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a simple explanation for each element of the line of code above:\n",
        "\n",
        "- `X_train, X_test, y_train, y_test`: These are the output variables. `X_train` and `y_train` are the training sets for the features and target variable, respectively, while `X_test` and `y_test` are the test sets for the features and target variable.\n",
        "\n",
        "- `train_test_split`: This is a function from the `sklearn.model_selection` module that splits arrays or matrices into random train and test subsets.\n",
        "\n",
        "- `X`: This represents the features of the dataset. It is the input data without the target variable.\n",
        "\n",
        "- `y`: This represents the target variable or the labels of the dataset. It is the output data we want to predict.\n",
        "\n",
        "- `test_size=0.3`: This parameter specifies the proportion of the dataset to include in the test split. Here, 30% of the data will be used for testing.\n",
        "\n",
        "- `random_state=42`: This parameter sets the seed for the random number generator, ensuring that the split of the data into training and test sets is reproducible. Using a specific value (like 42) ensures that the same split will occur each time the code is run."
      ],
      "metadata": {
        "id": "g47rmM51I9ES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sePTYwPMZGuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Build  "
      ],
      "metadata": {
        "id": "QSE3_pHnGqHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to accomplish the following:\n",
        "  \n",
        "- `model = LogisticRegression()`: This line creates an instance of the `LogisticRegression` class. This model is used for predicting binary outcomes (e.g., whether an employee will leave or stay).\n",
        "  \n",
        "- `model.fit(X_train, y_train)`: This line trains the logistic regression model using the training data. `X_train` contains the features (input variables), and `y_train` contains the target variable (output). The `fit` method adjusts the model parameters to best fit the training data, learning the relationship between the features and the target variable.  \n",
        "  "
      ],
      "metadata": {
        "id": "lu_t-iAhJ6rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pipline to preprocess data and train the model\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KGDHCI8pFyvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gppKhXS7ZDip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation of Results  \n"
      ],
      "metadata": {
        "id": "ldX0E6NEJi6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what each part of the following code does and how to interpret the results:\n",
        "\n",
        "- `y_pred = model.predict(X_test)`: This line uses the trained logistic regression model to make predictions on the test data (`X_test`). The predicted values (`y_pred`) indicate whether the model thinks each test instance belongs to the positive class (e.g., an employee will leave) or the negative class (e.g., an employee will stay).\n",
        "\n",
        "- `print(\"Accuracy:\", accuracy_score(y_test, y_pred))`: This line calculates and prints the accuracy of the model, which is the proportion of correct predictions out of the total predictions. Accuracy is a measure of how often the model correctly predicts the target variable.\n",
        "\n",
        "- `print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))`: This line prints the confusion matrix, which is a table showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. It helps to understand how well the model distinguishes between the two classes. The matrix is interpreted as follows:\n",
        "  - TP: The model correctly predicted the positive class.\n",
        "  - TN: The model correctly predicted the negative class.\n",
        "  - FP: The model incorrectly predicted the positive class (false alarm).\n",
        "  - FN: The model incorrectly predicted the negative class (missed detection).\n",
        "\n",
        "- `print(\"Classification Report:\\n\", classification_report(y_test, y_pred))`: This line prints the classification report, which includes several important metrics:\n",
        "  - **Precision**: The proportion of positive predictions that are actually correct (TP / (TP + FP)).\n",
        "  - **Recall**: The proportion of actual positives that are correctly identified (TP / (TP + FN)).\n",
        "  - **F1-Score**: The harmonic mean of precision and recall, providing a single metric that balances both concerns (2 * (Precision * Recall) / (Precision + Recall)).\n",
        "  - **Support**: The number of actual occurrences of each class in the test set.\n",
        "\n",
        "Interpreting these metrics provides a comprehensive understanding of the model's performance, highlighting its strengths and weaknesses in making predictions."
      ],
      "metadata": {
        "id": "9-rKfhLrKv1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the logistic regression model from the pipeline\n",
        "model = pipeline.named_steps['model']\n",
        "\n",
        "# Predict and evaluate\n",
        "print(\"Model accuracy on test set:\", pipeline.score(X_test, y_test))\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "T_0Bjm_rJo0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R-f1MIwCZCHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Analytic Findings  "
      ],
      "metadata": {
        "id": "5nWrHkBLYled"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Analysis\n",
        "\n",
        "#### Updated Analytic Summary\n",
        "\n",
        "**Model Performance Overview**:\n",
        "After transforming the features into their appropriate numeric and categorical formats, the logistic regression model achieves an improved accuracy of approximately 87.98% on the test dataset. This indicates that the model correctly predicts the employee attrition status (leave or stay) 87.98% of the time.\n",
        "\n",
        "**Detailed Accuracy**:\n",
        "The detailed accuracy remains at approximately 87.07%, consistent with the previously reported results, suggesting stable performance across different runs.\n",
        "\n",
        "**Confusion Matrix Analysis**:\n",
        "- **True Negatives (TN)**: 360\n",
        "- **False Positives (FP)**: 20\n",
        "- **False Negatives (FN)**: 37\n",
        "- **True Positives (TP)**: 24\n",
        "\n",
        "The confusion matrix shows that the model continues to perform strongly in identifying employees who will stay (True Negatives) and has moderate success in correctly identifying those who will leave (True Positives).\n",
        "\n",
        "**Classification Report**:\n",
        "- **Class 0 (Employees who stay)**:\n",
        "  - Precision: 0.91\n",
        "  - Recall: 0.95\n",
        "  - F1-Score: 0.93\n",
        "  - Support: 380\n",
        "\n",
        "The model is highly effective at predicting employees who will stay, with high precision (91%) and recall (95%), leading to a strong F1-score of 93%.\n",
        "\n",
        "- **Class 1 (Employees who leave)**:\n",
        "  - Precision: 0.55\n",
        "  - Recall: 0.39\n",
        "  - F1-Score: 0.46\n",
        "  - Support: 61\n",
        "\n",
        "The model's performance for predicting employees who will leave is moderate, with a precision of 55% and recall of 39%, resulting in an F1-score of 46%. This indicates that while the model correctly identifies 55% of employees predicted to leave, it only captures 39% of all employees who actually leave.\n",
        "\n",
        "**Averages**:\n",
        "- **Macro Average**:\n",
        "  - Precision: 0.73\n",
        "  - Recall: 0.67\n",
        "  - F1-Score: 0.69\n",
        "\n",
        "The macro average provides a balanced metric, treating both classes equally without considering their respective frequencies.\n",
        "\n",
        "- **Weighted Average**:\n",
        "  - Precision: 0.86\n",
        "  - Recall: 0.87\n",
        "  - F1-Score: 0.86\n",
        "\n",
        "The weighted average takes into account the support (frequency) of each class, providing an overall performance measure that aligns closely with the accuracy of the model.\n"
      ],
      "metadata": {
        "id": "XSAiUvQENace"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model, after transforming features into numeric and categorical formats, demonstrates improved accuracy and consistent performance in predicting employee attrition. While the model is highly effective in predicting employees who will stay, there is room for improvement in predicting employees who will leave. Further tuning of the model, additional features, or exploring different algorithms could help enhance recall and F1-score for the employees predicted to leave."
      ],
      "metadata": {
        "id": "hlcujicoY6KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Analysis  \n"
      ],
      "metadata": {
        "id": "OSa4yANyVofi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feauture Coefficients  \n",
        "  "
      ],
      "metadata": {
        "id": "G4e8CkqDXb9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code analyzes the features used in the model.\n",
        "  \n",
        "`coefficients = model.coef_[0]`: Extract the coefficients of the logistic regression model.  `\n",
        "\n",
        "`features = X.columns`: Get the feature names.  \n",
        "  \n",
        "`plt.barh(features, coefficients, align=\"center\")`: Plot the coefficients as a bar chart.  \n",
        "  \n",
        "The table `feature_importance` provides a sorted view of the features and their corresponding coefficients, making it easier to interpret the importance of each feature."
      ],
      "metadata": {
        "id": "Qdha_LLDVtlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names after transformation\n",
        "encoded_features = pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_features)\n",
        "all_features = numeric_features + encoded_features.tolist()\n",
        "\n",
        "# Get the coefficients and feature names\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Plot the coefficients\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature Coefficients\")\n",
        "y_pos = np.arange(len(all_features))\n",
        "plt.barh(y_pos, coefficients, align=\"center\", height=0.6)  # Adjust height for more spacing\n",
        "plt.yticks(y_pos, all_features)\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.grid(axis='x')\n",
        "plt.show()\n",
        "\n",
        "# Display the coefficients in a table for better readability\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': all_features,\n",
        "    'Coefficient': coefficients\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "id": "-XGjLZt6VIb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Features by Impact     \n",
        "  "
      ],
      "metadata": {
        "id": "LynMG2EvXiAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many feautures included in the model.  Let's look at the most impactful features that positively impact employee attrition, and let's look at the features that have the greatest negative impact on employee attrition (our target value).  "
      ],
      "metadata": {
        "id": "U3nqDYQIX_rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names after transformation\n",
        "encoded_features = pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(categorical_features)\n",
        "all_features = numeric_features + encoded_features.tolist()\n",
        "\n",
        "# Get the coefficients and feature names\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame for the coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': all_features,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by the coefficient values\n",
        "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "# List the most impactful positive and negative coefficient features\n",
        "print(\"Most Impactful Positive Coefficient Features:\")\n",
        "print(feature_importance.head())\n",
        "\n",
        "print(\"\\nMost Impactful Negative Coefficient Features:\")\n",
        "print(feature_importance.tail())"
      ],
      "metadata": {
        "id": "10wv6CO3Xm3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "coghzBjPF2U1"
      }
    }
  ]
}