{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL2CxPMVvs15qI5s4c+7aD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CIS9557_bctw/blob/main/cis9557_IBM_HR_attrition_dataset_asObjects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><font color=red>THIS NOTEBOOK'S MODEL USES ALL OBJECT(STRING) FEATURES</font>  \n",
        ""
      ],
      "metadata": {
        "id": "pdgMexHObMsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IBM HR Attrition dataset  \n",
        "\n",
        "<b>by Professor Patrick, June 2024</b>  \n",
        "  \n",
        "The IBM HR Analytics Employee Attrition & Performance dataset is commonly used to demonstrate various business analytics techniques.\n",
        "  \n",
        "This dataset includes features like employee age, attrition, department, distance from home, education, job involvement, job level, job satisfaction, and other relevant variables that can be analyzed to understand factors influencing employee retention and performance.  "
      ],
      "metadata": {
        "id": "eFlItC9zF5aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IBM HR Analytics Employee Attrition & Performance dataset is an excellent resource for developing a predictive analytics example. Here are several reasons why this dataset is particularly useful:  \n",
        "\n",
        "- <b>Real-World Relevance</b>: The dataset contains realistic and comprehensive employee data that mirrors what students might encounter in a real-world business environment.  \n",
        "  \n",
        "- <b>Diverse Features</b>: It includes various features (age, job role, department, salary, work-life balance, job satisfaction, etc.) that can be used to build complex models and understand different factors affecting employee attrition.  \n",
        "  \n",
        "- <b>Predictive Modelling</b>: Students can apply various predictive analytics techniques such as logistic regression, decision trees, random forests, and support vector machines to predict employee attrition.  \n",
        "  \n",
        "- <b>Data Preprocessing</b>: The dataset offers opportunities to teach students about data cleaning, feature selection, and transformationâ€”key steps in any predictive analytics project.  \n",
        "  \n",
        "- <b>Interpretability</b>: The results from the predictive models can be interpreted and communicated effectively, which is essential for making data-driven business decisions.  \n"
      ],
      "metadata": {
        "id": "33A7OIU4Gylr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gKOnFo0KZPpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Housekeeping  "
      ],
      "metadata": {
        "id": "zSOtCOhkGZF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s43QrxuzLGJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AUWf-_BBZNqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sourcing Data"
      ],
      "metadata": {
        "id": "91XRS5_gLKbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source the dataset\n",
        "!curl https://raw.githubusercontent.com/ProfessorPatrickSlatraigh/data/main/IBM_Fn-UseC_-HR-Employee-Attrition.csv -o \"IBM_HR_Employee_Attrition.csv\"\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('IBM_HR_Employee_Attrition.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "X = data.drop('Attrition_Yes', axis=1)\n",
        "y = data['Attrition_Yes']"
      ],
      "metadata": {
        "id": "yzwR-JNKFdeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "27YSttiTZMZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive Analytics  \n",
        "  \n",
        "Here's some code to perform a simple descriptive analysis of the `IBM HR Analytics Employee Attrition & Performance dataset`. This analysis includes loading the dataset, displaying basic information, checking for missing values, and summarizing key statistics for numerical and categorical features.  \n",
        "\n",
        "\n",
        "1. **Displaying the First Few Rows**:\n",
        "   - `print(data.head())`: This prints the first few rows of the dataset to give an initial view of the data.\n",
        "\n",
        "2. **Basic Information**:\n",
        "   - `print(data.info())`: This provides a summary of the dataset, including the number of non-null entries and data types of each column.\n",
        "\n",
        "3. **Checking for Missing Values**:\n",
        "   - `print(data.isnull().sum())`: This checks and displays the number of missing values in each column.\n",
        "\n",
        "4. **Summary Statistics for Numerical Features**:\n",
        "   - `print(data.describe())`: This provides summary statistics (count, mean, std, min, 25%, 50%, 75%, max) for the numerical features.\n",
        "\n",
        "5. **Summary Statistics for Categorical Features**:\n",
        "   - `print(data.describe(include=['object']))`: This provides summary statistics for categorical features, including the count, unique values, top value, and frequency.\n",
        "\n",
        "6. **Distribution of the Target Variable (Attrition)**:\n",
        "   - `print(data['Attrition'].value_counts())`: This displays the distribution of the target variable, showing the counts of employees who stayed and left.\n",
        "\n",
        "7. **Distribution of Categorical Features**:\n",
        "   - A loop iterates over each categorical feature to display its distribution using `value_counts()`.\n"
      ],
      "metadata": {
        "id": "BRVcnQ87RkET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Exploration  \n"
      ],
      "metadata": {
        "id": "k_ZOfUvuSNv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nBasic information about the dataset:\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "id": "oo9Y9KW_R6Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descriptive Analysis - Counts   \n",
        "  "
      ],
      "metadata": {
        "id": "Xq4gOPRxSWIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values in the dataset:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Display summary statistics for numerical features\n",
        "print(\"\\nSummary statistics for numerical features:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Display summary statistics for categorical features\n",
        "print(\"\\nSummary statistics for categorical features:\")\n",
        "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
        "for feature in categorical_features:\n",
        "    print(f\"\\n{feature} distribution:\")\n",
        "    print(data[feature].value_counts())\n",
        "if categorical_features.empty:\n",
        "    print('No categorical features found.')\n"
      ],
      "metadata": {
        "id": "26sPBC49SalS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Target Variable"
      ],
      "metadata": {
        "id": "H2xKUE8nSfo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the distribution of the target variable (Attrition)\n",
        "print(\"\\nDistribution of the target variable (Attrition):\")\n",
        "print(data['Attrition'].value_counts())"
      ],
      "metadata": {
        "id": "ZNyednUMStR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exploring Categorical Variables  \n",
        "  "
      ],
      "metadata": {
        "id": "8fgPU1DnSxQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the distribution of categorical features\n",
        "print(\"\\nDistribution of categorical features:\")\n",
        "categorical_features = data.select_dtypes(include=['object']).columns\n",
        "for feature in categorical_features:\n",
        "    print(f\"\\n{feature} distribution:\")\n",
        "    print(data[feature].value_counts())"
      ],
      "metadata": {
        "id": "OTVbB02LS0ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eoTB3Dr3ZIDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare Training Set  "
      ],
      "metadata": {
        "id": "ybwH2m38GjoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reserve 30% of the data to be used as the test set, while the remaining 70% will be used as the training set.  In other words, 70% of the data will be used to develop the model and the remaining 30% of the data can be used to test the model."
      ],
      "metadata": {
        "id": "bkepvDGkIuJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "2UJwofh5FiJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a simple explanation for each element of the line of code above:\n",
        "\n",
        "- `X_train, X_test, y_train, y_test`: These are the output variables. `X_train` and `y_train` are the training sets for the features and target variable, respectively, while `X_test` and `y_test` are the test sets for the features and target variable.\n",
        "\n",
        "- `train_test_split`: This is a function from the `sklearn.model_selection` module that splits arrays or matrices into random train and test subsets.\n",
        "\n",
        "- `X`: This represents the features of the dataset. It is the input data without the target variable.\n",
        "\n",
        "- `y`: This represents the target variable or the labels of the dataset. It is the output data we want to predict.\n",
        "\n",
        "- `test_size=0.3`: This parameter specifies the proportion of the dataset to include in the test split. Here, 30% of the data will be used for testing.\n",
        "\n",
        "- `random_state=42`: This parameter sets the seed for the random number generator, ensuring that the split of the data into training and test sets is reproducible. Using a specific value (like 42) ensures that the same split will occur each time the code is run."
      ],
      "metadata": {
        "id": "g47rmM51I9ES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sePTYwPMZGuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformations  \n"
      ],
      "metadata": {
        "id": "hQm3kKQYGnGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "WS2z0NioFtlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can break down the code above into the following:\n",
        "\n",
        "- `scaler = StandardScaler()`: This line creates an instance of the `StandardScaler` class. The `StandardScaler` is a tool used to standardize features by removing the mean and scaling to unit variance, which is useful for ensuring that each feature contributes equally to the model.\n",
        "\n",
        "- `X_train = scaler.fit_transform(X_train)`: This line first fits the `StandardScaler` to the training data (`X_train`). During this process, it calculates the mean and standard deviation for each feature in the training set. Then, it transforms the training data by subtracting the mean and dividing by the standard deviation, effectively standardizing the features.\n",
        "\n",
        "- `X_test = scaler.transform(X_test)`: This line transforms the test data (`X_test`) using the same mean and standard deviation calculated from the training data. This ensures that the test data is scaled in the same way as the training data, maintaining consistency between the datasets."
      ],
      "metadata": {
        "id": "f2RRaBWYJTzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uiFG3AIzZFZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Build  "
      ],
      "metadata": {
        "id": "QSE3_pHnGqHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to accomplish the following:\n",
        "  \n",
        "- `model = LogisticRegression()`: This line creates an instance of the `LogisticRegression` class. This model is used for predicting binary outcomes (e.g., whether an employee will leave or stay).\n",
        "  \n",
        "- `model.fit(X_train, y_train)`: This line trains the logistic regression model using the training data. `X_train` contains the features (input variables), and `y_train` contains the target variable (output). The `fit` method adjusts the model parameters to best fit the training data, learning the relationship between the features and the target variable.  \n",
        "  "
      ],
      "metadata": {
        "id": "lu_t-iAhJ6rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KGDHCI8pFyvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gppKhXS7ZDip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation of Results  \n"
      ],
      "metadata": {
        "id": "ldX0E6NEJi6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what each part of the following code does and how to interpret the results:\n",
        "\n",
        "- `y_pred = model.predict(X_test)`: This line uses the trained logistic regression model to make predictions on the test data (`X_test`). The predicted values (`y_pred`) indicate whether the model thinks each test instance belongs to the positive class (e.g., an employee will leave) or the negative class (e.g., an employee will stay).\n",
        "\n",
        "- `print(\"Accuracy:\", accuracy_score(y_test, y_pred))`: This line calculates and prints the accuracy of the model, which is the proportion of correct predictions out of the total predictions. Accuracy is a measure of how often the model correctly predicts the target variable.\n",
        "\n",
        "- `print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))`: This line prints the confusion matrix, which is a table showing the number of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. It helps to understand how well the model distinguishes between the two classes. The matrix is interpreted as follows:\n",
        "  - TP: The model correctly predicted the positive class.\n",
        "  - TN: The model correctly predicted the negative class.\n",
        "  - FP: The model incorrectly predicted the positive class (false alarm).\n",
        "  - FN: The model incorrectly predicted the negative class (missed detection).\n",
        "\n",
        "- `print(\"Classification Report:\\n\", classification_report(y_test, y_pred))`: This line prints the classification report, which includes several important metrics:\n",
        "  - **Precision**: The proportion of positive predictions that are actually correct (TP / (TP + FP)).\n",
        "  - **Recall**: The proportion of actual positives that are correctly identified (TP / (TP + FN)).\n",
        "  - **F1-Score**: The harmonic mean of precision and recall, providing a single metric that balances both concerns (2 * (Precision * Recall) / (Precision + Recall)).\n",
        "  - **Support**: The number of actual occurrences of each class in the test set.\n",
        "\n",
        "Interpreting these metrics provides a comprehensive understanding of the model's performance, highlighting its strengths and weaknesses in making predictions."
      ],
      "metadata": {
        "id": "9-rKfhLrKv1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "T_0Bjm_rJo0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R-f1MIwCZCHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Anlaytic Findings  "
      ],
      "metadata": {
        "id": "5nWrHkBLYled"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Analysis\n",
        "\n",
        "**Model Performance Overview**:\n",
        "The logistic regression model achieves approximately 87.1% accuracy on the test dataset. This indicates that it correctly predicts employee attrition status (leave or stay) 87.1% of the time.\n",
        "\n",
        "**Confusion Matrix Analysis**:\n",
        "\n",
        "- True Negatives (TN): 360\n",
        "- False Positives (FP): 20\n",
        "- False Negatives (FN): 37\n",
        "- True Positives (TP): 24\n",
        "\n",
        "The confusion matrix suggests that the model effectively identifies employees who will stay (True Negatives), but it has more difficulty identifying those who will leave (True Positives).\n",
        "\n",
        "**Classification Report**:\n",
        "\n",
        "- **For employees who stay (False)**:\n",
        "    - Precision: 0.91\n",
        "    - Recall: 0.95\n",
        "    - F1-Score: 0.93\n",
        "    - Support: 380\n",
        "\n",
        "The model is highly accurate at predicting employees who will stay, as shown by a high precision (91%) and recall (95%), resulting in a strong F1-score (93%).\n",
        "\n",
        "- **For employees who leave (True)**:\n",
        "    - Precision: 0.55\n",
        "    - Recall: 0.39\n",
        "    - F1-Score: 0.46\n",
        "    - Support: 61\n",
        "\n",
        "The model is less effective at predicting employees who will leave, with a precision of 55% and recall of 39%, which results in a lower F1-score of 46%. This suggests that while the model correctly identifies 55% of employees predicted to leave, it only captures 39% of all employees who actually leave.\n",
        "\n",
        "**Averages**:\n",
        "\n",
        "- **Macro Average**:\n",
        "    - Precision: 0.73\n",
        "    - Recall: 0.67\n",
        "    - F1-Score: 0.69\n",
        "\n",
        "The macro average treats both classes equally without considering their respective frequencies, providing a balanced metric.\n",
        "\n",
        "- **Weighted Average**:\n",
        "    - Precision: 0.86\n",
        "    - Recall: 0.87\n",
        "    - F1-Score: 0.86\n",
        "\n",
        "The weighted average takes into account the support (frequency) of each class, providing a performance measure that closely aligns with the model's accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "XSAiUvQENace"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model has high accuracy and performs well in predicting employees who will stay. However, it's less effective when predicting employees who will leave. To improve the recall and F1-score for the True class, further tuning of the model or exploration of additional features that could better capture the characteristics of employees likely to leave might be necessary."
      ],
      "metadata": {
        "id": "hlcujicoY6KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Analysis  \n"
      ],
      "metadata": {
        "id": "OSa4yANyVofi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feauture Coefficients  \n",
        "  "
      ],
      "metadata": {
        "id": "G4e8CkqDXb9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code analyzes the features used in the model.\n",
        "  \n",
        "`coefficients = model.coef_[0]`: Extract the coefficients of the logistic regression model.  `\n",
        "\n",
        "`features = X.columns`: Get the feature names.  \n",
        "  \n",
        "`plt.barh(features, coefficients, align=\"center\")`: Plot the coefficients as a bar chart.  \n",
        "  \n",
        "The table `feature_importance` provides a sorted view of the features and their corresponding coefficients, making it easier to interpret the importance of each feature."
      ],
      "metadata": {
        "id": "Qdha_LLDVtlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients and feature names\n",
        "coefficients = model.coef_[0]\n",
        "features = X.columns\n",
        "\n",
        "# Plot the coefficients\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature Coefficients\")\n",
        "y_pos = np.arange(len(features))\n",
        "plt.barh(y_pos, coefficients, align=\"center\", height=1.0)  # Adjust height for more spacing\n",
        "plt.yticks(y_pos, features)\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.grid(axis='x')\n",
        "plt.show()\n",
        "\n",
        "# Display the coefficients in a table for better readability\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Coefficient': coefficients\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "id": "-XGjLZt6VIb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Features by Impact     \n",
        "  "
      ],
      "metadata": {
        "id": "LynMG2EvXiAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many feautures included in the model.  Let's look at the most impactful features that positively impact employee attrition, and let's look at the features that have the greatest negative impact on employee attrition (our target value).  "
      ],
      "metadata": {
        "id": "U3nqDYQIX_rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for the coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by the coefficient values\n",
        "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
        "\n",
        "# List the most impactful positive and negative coefficient features\n",
        "print(\"Most Impactful Positive Coefficient Features:\")\n",
        "print(feature_importance.head())\n",
        "\n",
        "print(\"\\nMost Impactful Negative Coefficient Features:\")\n",
        "print(feature_importance.tail())"
      ],
      "metadata": {
        "id": "10wv6CO3Xm3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "coghzBjPF2U1"
      }
    }
  ]
}